{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## math, plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "## machine learning\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "import RateMapClass_William_20190308 as RateMapClass\n",
    "import ratterdam_RepetitionCoreFx as CoreFx\n",
    "import confounds as direction\n",
    "import newAlleyBounds as bounds2\n",
    "\n",
    "%matplotlib qt5\n",
    "%qtconsole --style paraiso-dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_scorer(x,y, inst_fr):\n",
    "    target1 = (x-np.mean(x))/np.std(x)\n",
    "    target2 = (y-np.mean(y))/np.std(y)\n",
    "\n",
    "    train_size = 8000\n",
    "    test_size = 1000\n",
    "    buffer = int((dt-train_size-test_size)/2)\n",
    "    for i in range(0, int(len(inst_fr)/dt)):\n",
    "        if i == 0:\n",
    "            X_train = inst_fr[i*dt : i*dt + train_size,:]\n",
    "            y_train1 = target1[i*dt : i*dt + train_size]\n",
    "            y_train2 = target2[i*dt : i*dt + train_size]\n",
    "            X_test = inst_fr[(i)*dt + train_size + buffer: (i)*dt + train_size + buffer+test_size,:]\n",
    "            y_test1 = target1[(i)*dt + train_size + buffer: (i)*dt + train_size + buffer+test_size]\n",
    "            y_test2 = target2[(i)*dt + train_size + buffer: (i)*dt + train_size + buffer+test_size]\n",
    "        else:\n",
    "            X_train  = np.vstack((X_train, inst_fr[i*dt + buffer: i*dt + train_size+ buffer]))\n",
    "            ##print('training', i*dt + buffer,i*dt + train_size+ buffer)\n",
    "            X_test = np.vstack((X_test, inst_fr[(i)*dt + train_size + 2*buffer: (i)*dt + train_size + 2*buffer+test_size,:]))\n",
    "            ##print('testing',(i)*dt + train_size + 2*buffer,((i)*dt + train_size + 2*buffer+test_size))\n",
    "            y_train1 = np.append(y_train1, target1[i*dt+ buffer: i*dt + train_size+ buffer])\n",
    "            y_test1 = np.append(y_test1, target1[(i)*dt + train_size + 2*buffer: (i)*dt + train_size + 2*buffer+test_size])\n",
    "\n",
    "            y_train2 = np.append(y_train2, target2[i*dt+ buffer: i*dt + train_size+ buffer])\n",
    "            y_test2 = np.append(y_test2, target2[(i)*dt + train_size + 2*buffer: (i)*dt + train_size + 2*buffer+test_size])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)        \n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train1)\n",
    "\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train, y_train2)\n",
    "    \n",
    "    \n",
    "    train_score = (model.score(X_train, y_train1) + model2.score(X_train, y_train2))/2\n",
    "    test_score = (model.score(X_test, y_test1) + model2.score(X_test, y_test2))/2\n",
    "    ##print('model score', model.score(X_train, y_train1))\n",
    "    ##ypred = model.predict(X_train)\n",
    "    ##print('r2 function', r2_score(y_train1, ypred))\n",
    "    return(np.array([train_score, test_score]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R765 RFD5\n",
      "R765 DFD4\n",
      "R781 D3\n",
      "R781 D4\n",
      "R808 D6\n",
      "R808 D7\n",
      "R859 D1\n",
      "R859 D2\n",
      "R886 D1\n",
      "R886 D2\n"
     ]
    }
   ],
   "source": [
    "## Reading in file with all data\n",
    "with open(\"E:\\\\Ratterdam\\\\R_data_repetition\\\\20220405-124315_superPopulationRepetition.pickle\",\"rb\") as f:\n",
    "    alldat = pickle.load(f)   \n",
    "store_dir = \"E:\\\\UserData\\\\Documents\\\\GitHub\\\\ratterdam\\\\RatterdamOpen_Project\\\\repetition_manuscript_code\\\\figure6\\\\\"\n",
    "\n",
    "num_rep = 100\n",
    "num_down_samp = 10\n",
    "dt = 10000\n",
    "all_num_repeat= []\n",
    "all_num = []\n",
    "all_rats = list(alldat.keys())\n",
    "all_rat_day = []\n",
    "all_scores = []\n",
    "for rat in all_rats:  \n",
    "    all_days = list(alldat[rat].keys())\n",
    "    for day in all_days:\n",
    "        print(rat, day)\n",
    "        day_scores = np.zeros((num_rep,2))\n",
    "        for rep in range(0,num_rep):\n",
    "            all_rat_day.append(rat+ '_' + day)\n",
    "            inst_fr = np.load(store_dir + rat + '_' + day + '_inst_fr.npy')\n",
    "            x_order = np.load(store_dir + rat + '_' + day + '_x_within.npy')\n",
    "            y_order = np.load(store_dir + rat + '_' + day + '_y_within.npy')\n",
    "            \n",
    "            shuff_start = np.random.choice(len(inst_fr),1)[0]\n",
    "            shuff_inst_fr = np.vstack((inst_fr[shuff_start:,:], inst_fr[0:shuff_start,:]))\n",
    "            inst_fr = shuff_inst_fr\n",
    "            x = np.concatenate((x_order[shuff_start:], x_order[0:shuff_start]))\n",
    "            y = np.concatenate((y_order[shuff_start:], y_order[0:shuff_start]))\n",
    "            \n",
    "            repeating_cells = []\n",
    "            all_cells = alldat[rat][day]['units'].keys()\n",
    "            for each_cell in all_cells:\n",
    "                data = alldat[rat][day]['units'][each_cell]\n",
    "                if data.repeating == True:\n",
    "                    repeating_cells.append(True)\n",
    "                else:\n",
    "                    repeating_cells.append(False)\n",
    "                    \n",
    "            repeating_cells = np.array(repeating_cells)\n",
    "            repeat_len = np.sum(repeating_cells)\n",
    "            non_repeat_len = len(repeating_cells)- repeat_len\n",
    "            \n",
    "            if repeat_len<non_repeat_len:\n",
    "                inst_fr_rep = inst_fr[:,repeating_cells]\n",
    "                if len(inst_fr_rep[0])>0:\n",
    "                    curr_score = np.array(train_test_scorer(x,y, inst_fr_rep))\n",
    "                    \n",
    "                    scores_non_rep = np.zeros((num_down_samp,2))\n",
    "                    for down_sampling in range(0, num_down_samp):\n",
    "                        cells_chosen = np.random.choice(np.where(repeating_cells== False)[0],repeat_len)\n",
    "                        inst_fr_non = inst_fr[:,cells_chosen]\n",
    "                        scores_non_rep[down_sampling,:] = train_test_scorer(x,y, inst_fr_non)\n",
    "\n",
    "                    day_scores[rep,:] = curr_score-np.mean(scores_non_rep, axis = 0)\n",
    "                    ##print(curr_score,np.mean(scores_non_rep, axis = 0))\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                cells_chosen = np.random.choice(np.where(repeating_cells== True)[0],non_repeat_len)\n",
    "                inst_fr_rep = inst_fr[:,cells_chosen]\n",
    "                curr_score = np.array(train_test_scorer(x,y, inst_fr_rep))\n",
    "                inst_fr_non =inst_fr[:,repeating_cells==False]\n",
    "                day_scores[rep,:] = curr_score-np.array(train_test_scorer(x,y, inst_fr_non))\n",
    "                \n",
    "        all_scores.append(day_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-635fcdbd3b4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mall_scores_edit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "## Figure plotting\n",
    "\n",
    "for i in range(0, len(all_scores)):\n",
    "    if np.sum(all_scores[i,:,0] == 0.0) == len(all_scores):\n",
    "        print('doing')\n",
    "        all_scores_edit = np.delete(all_scores[:,:,0], i)\n",
    "all_scores = all_scores_edit\n",
    "\n",
    "all_rat_day = np.array(all_rat_day)\n",
    "all_rat_day = all_rat_day[all_rat_day!='R808_D7']\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "score_df = pd.DataFrame(all_scores[:,:,0].flatten(), columns = ['score'])\n",
    "score_df['rat-day'] = np.array(all_rat_day)\n",
    "score_df['train-test'] = ['train']*len(all_rat_day)\n",
    "\n",
    "score_df2= pd.DataFrame(all_scores[:,:,1].flatten(), columns = ['score'])\n",
    "score_df2['rat-day'] = np.array(all_rat_day)\n",
    "score_df2['train-test'] = ['test']*len(all_rat_day)\n",
    "\n",
    "score_df_all = pd.concat([score_df, score_df2])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sb.boxplot(x = score_df_all['rat-day'], y = score_df_all['score'], hue = score_df_all['train-test'])\n",
    "ax.set_xticklabels(np.unique(all_rat_day),rotation = 45)\n",
    "ax.set_ylabel('position decoding score difference')\n",
    "\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=0.9230303218521855, pvalue=0.382986076064611)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Statistics\n",
    "## Test if mean is equal to 0\n",
    "all_means = np.mean(all_scores[:,:,1], axis = 1)\n",
    "scipy.stats.ttest_1samp(all_means,0)\n",
    "\n",
    "## pvalue>0.05\n",
    "## we cannot reject the null that the scores are different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rat-day mean diff scores ttest pvalue- diff from 0\n",
      "R765_DFD4 0.0308 1.69e-08\n",
      "R765_RFD5 -0.133 4.28e-15\n",
      "R781_D3 -0.0643 1.91e-26\n",
      "R781_D4 -0.0661 6.76e-31\n",
      "R808_D6 0.59 0.00324\n",
      "R859_D1 -0.0453 0.000106\n",
      "R859_D2 0.204 1.96e-54\n",
      "R886_D1 0.0403 3.79e-11\n",
      "R886_D2 0.0491 8.09e-13\n"
     ]
    }
   ],
   "source": [
    "## Statistics\n",
    "## Test if mean is equal to 0\n",
    "all_pval = []\n",
    "print('rat-day', 'mean diff scores', 'ttest pvalue- diff from 0')\n",
    "for i in range(len(all_scores)):\n",
    "    if np.sum(all_scores[i,:,1]==0.0) != len(all_scores[i,:,1]):\n",
    "        curr_pval = scipy.stats.ttest_1samp(all_scores[i,:,1],0).pvalue\n",
    "        all_pval.append(curr_pval)\n",
    "        print(np.unique(all_rat_day)[i], f'{np.mean(all_scores[i,:,1]):.3}', f'{curr_pval:.3}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "810497aa9f218318f89d4d2cff33f7d283ecef935a2d4dd2910844a97859f315"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
